{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the cleaned data (see Preprocessing.py for more info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13850 new datasets created\n",
      "2211 new datasets created\n",
      "4170 new datasets created\n",
      "[[-1.68042768e+00 -1.56010796e+00  8.59134976e-01 -1.02436036e+00\n",
      "   0.00000000e+00  0.00000000e+00 -1.05426000e+00  2.10557232e-01\n",
      "  -6.36238929e-01 -3.55271368e-15  1.45262918e-01  4.17594861e-01\n",
      "  -7.65503633e-01 -9.84341635e-01 -2.22044605e-16 -1.83072601e-01\n",
      "   1.16133708e+00 -3.29715783e-01 -6.86822145e-01 -2.71492413e-01\n",
      "  -1.04083409e-17 -7.79025964e-01  0.00000000e+00  0.00000000e+00\n",
      "   1.00993203e+00  1.22886104e+00]\n",
      " [-1.68042768e+00 -1.54510745e+00 -1.96806288e+00  1.01354346e+00\n",
      "   0.00000000e+00  0.00000000e+00 -6.56093207e-01 -4.12476614e-01\n",
      "  -5.18405294e-01 -3.55271368e-15  1.45262918e-01  9.91149279e-01\n",
      "  -2.00677256e-01 -5.99141154e-01 -2.22044605e-16 -1.00480572e+00\n",
      "   1.35105016e+00 -8.86316125e-01 -5.99233539e-01 -6.44189828e-01\n",
      "  -1.04083409e-17 -2.06368919e+00  0.00000000e+00  0.00000000e+00\n",
      "   7.33612824e-01  4.97855968e-01]\n",
      " [-1.68042768e+00 -1.53010693e+00  3.11935390e-01 -5.40844964e-03\n",
      "   0.00000000e+00  0.00000000e+00 -6.56093207e-01 -1.25837113e+00\n",
      "  -7.77416964e-01 -3.55271368e-15  1.45262918e-01  1.20482641e+00\n",
      "   2.22942528e-01 -7.49399741e-01 -2.22044605e-16 -1.52772679e+00\n",
      "   1.94729125e+00 -1.90565697e-01 -5.69030572e-01 -1.96460353e+00\n",
      "  -1.04083409e-17 -7.79025964e-01  0.00000000e+00  0.00000000e+00\n",
      "   3.46765936e-01  7.71292373e-01]]\n"
     ]
    }
   ],
   "source": [
    "from Preprocessing import getData\n",
    "import numpy as np\n",
    "\n",
    "x_train,x_val,x_test,y_train,y_val,y_test = getData()\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 13850\t val data : 2211\t test data: 4170\n",
      "Classes and count for x_train: {0: 3570, 1: 6914, 2: 3366}\n",
      "(13850, 3, 26)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(x_train)}\\t val data : {len(x_val)}\\t test data: {len(x_test)}\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"Classes and count for x_train: {dict(zip(unique, counts))}\")\n",
    "print(x_train.shape)\n",
    "most_probable_class = np.argmax(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13850, 3, 26)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "# model.add(layers.Conv1D(32, 3, activation='relu',input_shape=(x_train[0].shape)))\n",
    "model = Sequential()\n",
    "# model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.Dense(26, activation= \"relu\"))\n",
    "model.add(layers.LSTM(units=20, return_sequences=True))\n",
    "# model.add(layers.LSTM(units=26, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(units=3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:07<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation loss:  0.6317712068557739 \t Evaluation accuracy: 0.6616306900978088\n",
      "0.479136690647482\n"
     ]
    }
   ],
   "source": [
    "rollingWindow = 1000\n",
    "valWindow = 0\n",
    "step = 500\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0,len(x_train) - rollingWindow-valWindow,step)):\n",
    "    model.fit(x_train[i:i+rollingWindow], y_train[i:i+rollingWindow], epochs=1, verbose=0,validation_data=(x_train[i+rollingWindow:i+rollingWindow+valWindow], y_train[i+rollingWindow:i+rollingWindow+valWindow]))\n",
    "\n",
    "evaluatation = model.evaluate(x_test, y_test,verbose=0)\n",
    "\n",
    "\n",
    "print(f\"\\n\\nEvaluation loss:  {evaluatation[0]} \\t Evaluation accuracy: {evaluatation[1]}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "base_line = accuracy_score(y_test, [most_probable_class]*len(y_test))\n",
    "print(base_line)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbcd16870f9a89e46e760e6b55b5046d1285a9bd78dfb3ab1aff4c621d5f2caa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
